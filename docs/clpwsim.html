<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Ivory: Cross-Lingual Pairwise Similarity</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="assets/css/bootstrap.css" rel="stylesheet">
    <link href="assets/css/bootstrap-responsive.css" rel="stylesheet">
    <link href="assets/css/docs.css" rel="stylesheet">
    <link href="assets/js/google-code-prettify/prettify.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

  </head>

  <body data-spy="scroll" data-target=".bs-docs-sidebar">

    <!-- Navbar
    ================================================== -->
    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <button type="button" class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <div class="nav-collapse collapse">
            <ul class="nav">
              <li class="">
                <a href="../index.html">Home</a>
              </li>
              <li class="">
                <a href="./start.html">Getting Started</a>
              </li>
              <li class="">
                <a href="./publications.html">Publications</a>
              </li>
              <li class="active">
                <a href="./experiments.html">Experiments</a>
              </li>
              <li class="">
                <a href="./team.html">Team</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>

<!-- Subhead
================================================== -->
<header class="jumbotron subhead" id="overview">
  <div class="container">
    <h1>Ivory</h1>
    <p class="lead">A Hadoop toolkit for web-scale information retrieval research</p>
  </div>
</header>

  <div class="container">

<div class="page-header">
<h2>Cross-Lingual Pairwise Similarity</h2>
<h4>A step-by-step tutorial for German and English Wikipedia</h4>
</div>

<!-- last time these instructions were run:

commit ea06ae441257566a2637625df82674b8f1452655
Merge: 6dff282 63e7e88
Author: lintool <jimmylin@umd.edu>
Date:   Thu Jun 13 03:25:01 2013 -0400

-->

<p>Ivory can perform cross-lingual pairwise similarity on large collections. The
underlying scalable approach is implemented as a two-phase pipeline.
The first phase was introduced in Ture et al.'s SIGIR'11 paper 
<b><a href="publications/Ture_etal_SIGIR2011.pdf">No Free Lunch: Brute Force vs 
Locality-Sensitive Hashing for Cross-Lingual Pairwise Similarity</a></b> and
the second is described in Ture & Lin's NAACL'12 paper 
<b><a href="publications/Ture_Lin_NAACL-HLT2012.pdf">
Why Not Grab a Free Lunch? Mining Large Corpora for Parallel Sentences To Improve
Translation Modeling</a></b>.<p>
In this two-part tutorial, we'll take you through the entire process using Wikipedia as our corpus: 
Starting from scratch, we first show how to find similar German-English article pairs
(i.e., phase 1), and then illustrate how to extract German-English parallel text (i.e., phase 2). 
The output of phase 1 is used in phase 2, so you can skip to the 
<a href="bitext-tutorial.html">second part of the tutorial</a>
if you have already completed the first one.
</p>

<table><tr><td valign="top"><span class="label label-info">Info</span></td>
<td style="padding-left: 10px">
Our approach can be applied to any collection format that is supported by
<a href="http://lintool.github.com/Cloud9">Cloud<sup>9</sup></a>, and any
language supported by Ivory. As of 1/13/2013, Ivory supports the following languages: English, German, French, Spanish, Chinese, 
Arabic, Czech, and Turkish. Everything should work out of the box for these languages. See the
<a href="../docs/tokenization.html">tokenization guide</a> on how to add support for other languages.
<p>
</td>
</tr>
</table>

<h3>1. Downloading Wikipedia</h3>

<p>Wikipedia articles can be downloaded in XML format from
<a href="http://wikipedia.c3sl.ufpr.br/">here</a>. Wikipedia is offered in many languages,
and you can download the content in different granularities. However, we only need the main 
article content to run our algorithm.</p> 

<p>Let's download and unpack German and English Wikipedia collections to our local
machine:</p>

<pre class="code">
wget http://wikipedia.c3sl.ufpr.br/dewiki/20121215/dewiki-20121215-pages-articles.xml.bz2
bunzip2 dewiki-20121215-pages-articles.xml.bz2
wget http://wikipedia.c3sl.ufpr.br/enwiki/20121201/enwiki-20121201-pages-articles.xml.bz2
bunzip2 enwiki-20121201-pages-articles.xml.bz2
</pre>

<table><tr><td valign="top"><span class="label label-info">Tip</span></td>
<td style="padding-left: 10px">
It might be a good idea to compare the MD5 checksums of the downloaded files to the values 
<a href="http://wikipedia.c3sl.ufpr.br/enwiki/20121201/enwiki-20121201-md5sums.txt">here</a>
and <a href="http://wikipedia.c3sl.ufpr.br/dewiki/20121215/dewiki-20121215-md5sums.txt">here</a>
before moving further.
<p>
</td>
</tr>
</table>

<p>A peek into these files should reveal Wikipedia's XML format:</p>

<pre class="code">
$ less dewiki-20121215-pages-articles.xml
&lt;mediawiki ... xml:lang="de"&gt;
 &lt;siteinfo&gt;
  ...
 &lt;page&gt;
  ...
 &lt;/page&gt;
 &lt;page&gt;
  ...
 &lt;/page&gt;
&lt;/mediawiki&gt;
</pre>

<p>Now, we are ready to transfer the XML-formatted collections to HDFS:</p>

<pre class="code">
hdfs dfs -put *wiki*pages-articles.xml $wiki/raw/
</pre>

<h3>2. Preprocessing Wikipedia</h3>

<p>The standard preprocessing pipeline of Ivory is shown in detail 
<a href="../docs/pipeline.html">here</a>, where the goal is to represent 
each document as a vector of term weights.</p>

<p>For stop word removal and stemming, we need to upload data files 
to HDFS for tokenizers to load:</p>

<pre class="code">
hdfs dfs -mkdir wikidata
hdfs dfs -mkdir wikidata/token
hdfs dfs -put data/tokenizer/*de*stop* wikidata/token/
hdfs dfs -put data/tokenizer/*en*stop* wikidata/token/
hdfs dfs -put data/tokenizer/de-token.bin wikidata/token/
hdfs dfs -put data/tokenizer/en-token.bin wikidata/token/
hdfs dfs -put data/vocab/*de-en* wikidata/
hdfs dfs -put data/vocab/*en-de* wikidata/
</pre>

<table><tr><td valign="top"><span class="label label-info">Tip</span></td>
<td style="padding-left: 10px">
For languages that do not require model files, there is no need
to add the <code>*-token.bin</code> model file. See 
<a href="../docs/tokenization.html">tokenization guide</a> for details.
<p>
</td>
</tr>
</table>

<p>Since there are documents in two different languages in our case, we need to translate 
the representations into the same language space for comparison. We use the cross-language 
IR methods introduced in <i>Darwish and Oard</i>'s <b>Probabilistic Structured Query Methods (2003)</b>
to translate the German representations into English.
For this, we assume that vocabularies and translation tables
are available on some HDFS data directory. These can be generated using 
the <a href="hooka.html">Hooka word alignment toolkit</a> 
(also supports conversion from GIZA++ or BerkeleyAligner).</p>

<p>This should be how your HDFS directory looks like at this step:</p>
<pre class="code">
$ hdfs dfs -ls wikidata/
Found 7 items
drwxr-xr-x   - jimmylin supergroup          0 2013-06-13 03:31 wikidata/token
-rw-r--r--   3 jimmylin supergroup    3929563 2013-06-13 03:31 wikidata/ttable.de-en
-rw-r--r--   3 jimmylin supergroup    2059694 2013-06-13 03:31 wikidata/ttable.en-de
-rw-r--r--   3 jimmylin supergroup    2728544 2013-06-13 03:31 wikidata/vocab.de-en.de
-rw-r--r--   3 jimmylin supergroup     662751 2013-06-13 03:31 wikidata/vocab.de-en.en
-rw-r--r--   3 jimmylin supergroup    1336552 2013-06-13 03:31 wikidata/vocab.en-de.de
-rw-r--r--   3 jimmylin supergroup     754025 2013-06-13 03:31 wikidata/vocab.en-de.en

$ hdfs dfs -ls wikidata/token
Found 6 items
-rw-r--r--   3 jimmylin supergroup     851682 2013-06-13 03:30 wikidata/token/de-token.bin
-rw-r--r--   3 jimmylin supergroup       1349 2013-06-13 03:30 wikidata/token/de.stop
-rw-r--r--   3 jimmylin supergroup        685 2013-06-13 03:30 wikidata/token/de.stop.stemmed
-rw-r--r--   3 jimmylin supergroup     439890 2013-06-13 03:31 wikidata/token/en-token.bin
-rw-r--r--   3 jimmylin supergroup       5301 2013-06-13 03:30 wikidata/token/en.stop
-rw-r--r--   3 jimmylin supergroup       3685 2013-06-13 03:30 wikidata/token/en.stop.stemmed
</pre>

<p>Now we are ready to run the preprocessing pipeline. The first run is
for the English Wikipedia collection:</p>

<pre class="code">
nohup etc/hadoop-cluster.sh ivory.app.PreprocessWikipedia \
 -mode=crosslingE -xml=/shared/collections/wikipedia/raw/enwiki-20121201-pages-articles \
 -compressed=pwsim.enwiki.compressed -index=pwsim.enwiki.index -lang=en \
 -tokenizermodel=wikidata/token/en-token.bin \
 -collectionvocab=wikidata/vocab.de-en.en \
 -e_stopword=wikidata/token/en.stop >& pwsim.en.log &
</pre>

<table><tr><td valign="top"><span class="label label-info">Warning</span></td>
<td style="padding-left: 10px">
If the compressed Wikipedia collection already exists at <code>pwsim.enwiki.compressed/</code>,
the above program will re-use it. If you are not sure that the existing files are compatible with what you are 
running, remove or change them before running the above.
<p>
</td>
</tr>
</table>

<p>Next, we preprocess and translate German Wikipedia articles:</p>

<pre class="code">
nohup etc/hadoop-cluster.sh ivory.app.PreprocessWikipedia \
 -mode=crosslingF -xml=/shared/collections/wikipedia/raw/dewiki-20121215-pages-articles.xml \
 -compressed=pwsim.dewiki.compressed -index=pwsim.dewiki.index -targetindex=pwsim.enwiki.index \
 -lang=de -target_lang=en \
 -tokenizermodel=wikidata/token/de-token.bin \
 -e_tokenizermodel=wikidata/token/en-token.bin \
 -f_f2e_vocab=wikidata/vocab.de-en.de \
 -e_f2e_vocab=wikidata/vocab.de-en.en \
 -f2e_ttable=wikidata/ttable.de-en \
 -e_e2f_vocab=wikidata/vocab.en-de.en \
 -f_e2f_vocab=wikidata/vocab.en-de.de \
 -e2f_ttable=wikidata/ttable.en-de \
 -e_stopword=wikidata/token/en.stop \
 -f_stopword=wikidata/token/de.stop >& pwsim.de.log &
</pre>

<p>The log output of this program should gives various statistics, such as 
the number of documents, terms, tokens in the collection. Let's do a sanity
check to make sure everything ran correctly:</p>

<pre class="code">
$ grep -P 'TOTAL=|ARTICLE=|OTHER=|SumOfDocLengths=|Terms=|TransDf=|SHORT=|ZERO=|Total=|VOCAB SIZE|TransDf|DISAMB' pwsim.en.log | uniq
ARTICLE=4033137
DISAMBIGUATION=123666
NON_ARTICLE=2899446
OTHER=138350
TOTAL=12961996
SumOfDocLengths=970609403
Total=4033137
SumOfDocLengths=970609403
Terms=39431
Total=39431
SHORT=28619
Total=4004410
ZERO=108
Total=4004410
VOCAB SIZE 71644
</pre>

<p>You should see the <i>exact</i> same numbers in your log output.</p>

<pre class="code">
$ grep -P 'TOTAL=|ARTICLE=|OTHER=|SumOfDocLengths=|Terms=|TransDf=|SHORT=|ZERO=|Total=|VOCAB SIZE|TransDf|DISAMB' pwsim.de.log | uniq
ARTICLE=1326111
DISAMBIGUATION=174678
NON_ARTICLE=450258
OTHER=35564
TOTAL=3001626
SumOfDocLengths=328525410
Total=1326111
SumOfDocLengths=328525410
Terms=115368
Total=115368
SHORT=1183
Total=1324902
ZERO=7
Total=1324902
VOCAB SIZE 71644
</pre>


<h3>3. Representing Documents as Signatures</h3>
<p>Since we are done with preprocessing, we can now play with the document representation. 
It is possible to compress the content of a document vector substantially by computing
a <i>signature</i>, a bit/int array representation. Three different signature approaches 
are implemented as part of Ivory (details available in 
<a href="publications/Ture_etal_SIGIR2011.pdf">Ture et al's SIGIR'11 paper</a>):
<i>Simhash</i>, <i>min-hash</i>, and <i>randomized projections</i>. </p>

<p>Each of these methods has certain benefits and drawbacks, yet we have decided to use 
<i>random projections</i> in this application mainly due to its ability to have an 
arbitrary length (i.e., number of bits). The number of bits can be set empirically, 
by looking at the trade-off between effectiveness (i.e., how good a signature approximates
the document vector) and efficiency (i.e., time to compute similarity between two signatures) 
as we vary the number. We'll use 1000 bits in this tutorial:</p>

<pre class="code">
etc/hadoop-cluster.sh ivory.lsh.driver.RunComputeSignatures -index=pwsim.dewiki.index -num_bits=1000 -type=random
</pre>

<p>Since this process is based on 1000 randomly generated unit vectors, we need to use the same vectors when 
running this process for the English side. That's why we copy the random vectors over first:</p>

<pre class="code">
hdfs dfs -cp pwsim.dewiki.index/randomvectors_D=1000/ pwsim.enwiki.index/
</pre>

<p>And then run signature generation:</p>

<pre class="code">
etc/hadoop-cluster.sh ivory.lsh.driver.RunComputeSignatures -index=pwsim.enwiki.index -num_bits=1000 -type=random
</pre>

As a result of these jobs, you should see a <code>signatures-random_D=1000</code> folder under each index directory.

<h3>4. Finding Similar Document Pairs</h3>

<p>
In order to find similar Wikipedia articles, we will compare signatures by calculating the Hamming distance
(i.e., number of different bits), and returning any pair with less than a specified distance <code>T</code>. 
Instead of comparing all signatures of German Wikipedia to all signatures of English Wikipedia, which is called the 
brute-force approach, we have implemented the sliding window-based algorithm in Ravichandran et al.'s 
paper, <b>Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering</b>.
</p> 

<p>
According to this algorithm, we will generate <code>Q</code> permutations of each signature, resulting in <code>Q</code> 
signature tables. Each table is sorted with respect to the bit values, and then we compare each signature of each
table to its neighbors that are at most <code>B</code> positions away.
</p> 

<pre class="code">
nohup etc/hadoop-cluster.sh ivory.lsh.pwsim.GenerateChunkedPermutedTables \
 -sourceindex=pwsim.dewiki.index -index=pwsim.enwiki.index -num_bits=1000 -type=random -Q=300 -overlap=10000 -B=2000 >& chunking.log &

nohup etc/hadoop-cluster.sh ivory.lsh.pwsim.cl.CLSlidingWindowPwsim \
 -index=pwsim.enwiki.index -num_bits=1000 -type=random -Q=300 -overlap=10000 -B=2000 -T=400 >& windowing.log &
</pre>

<table><tr><td valign="top"><span class="label label-info">Info</span></td>
<td style="padding-left: 10px">
We perform a special optimization as the permuted signature tables are generated.
Each table is split into a number of consecutive <i>chunks</i> in order to increase
the degree of parallelism in the sliding window algorithm. The parameter 
<code>overlap</code> defines the amount of overlap between these chunks, and needs
to be at least as large as <code>B</code>.
<p>
</td>
</tr>
</table>

These two jobs may take a while to finish (e.g., few hours on a 128-core cluster), but if everything runs smoothly, 
you should see the output written to <code>$edir/similardocs_random_maxdst=400_D=1000_Q=300_B=2000</code>. 
The number of similar article pairs output by the algorithm will vary from run to run due to the randomized nature, 
but you should see tens of millions of German-English article pairs output (see counter <code>Reduce output records</code>) 
in the log output of <code>ivory.lsh.pwsim.cl.CLSlidingWindowPwsim</code>.

<h3>5. Evaluation</h3>
<p>
Ivory has many built-in functionalities for evaluation of pairwise similarity approaches. Let us describe the process
of evaluating the sliding window algorithm's output by comparing it to the brute-force approach. 
First, we'll sample about 1000 German Wikipedia articles for this evaluation:
</p>

<pre class="code">
etc/hadoop-cluster.sh ivory.lsh.eval.SampleIntDocVectors -index=pwsim.dewiki.index -size=1000 -docnos=pwsim.dewiki.index/sample-docnos
</pre>

<p>
At this point, around 1000 sampled document vectors should be written to <code>pwsim.dewiki.index/wt-int-doc-vectors_sample=1000</code>,
whereas the docnos identifying this sampled subset should be written to <code>pwsim.dewiki.index/sample-docnos</code>. We can do a quick
sanity check on that file, making sure that the following returns a number close to 1000 (not exactly due to random sampling):

<pre class="code">
hdfs dfs -cat pwsim.dewiki.index/sample-docnos | wc -l
</pre>

<p>
Next, we can filter the similar article pairs that do not appear in this sample:
</p>

<pre class="code">
etc/hadoop-cluster.sh ivory.lsh.eval.FilterResults \
 -input=pwsim.enwiki.index/similardocs_random_maxdst=400_D=1000_Q=300_B=2000 \
 -output=pwsim.enwiki.index/similardocs_random_maxdst=400_D=1000_Q=300_B=2000-filtered \
 -docnos=pwsim.dewiki.index/sample-docnos
</pre>

<p>Notice that the number of similar pairs was filtered down from tens of millions to tens of thousands.
We can compute comparable output using a brute-force approach (i.e., compare each sampled document vector from German 
Wikipedia to all document vectors of English Wikipedia):</p>

<pre class="code">
etc/hadoop-cluster.sh ivory.lsh.eval.BruteForcePwsim \
 -input=pwsim.enwiki.index/wt-int-doc-vectors \
 -sample=pwsim.dewiki.index/wt-int-doc-vectors_sample=1000/part-00000 \
 -output=pwsim.enwiki.index/groundtruth_T=0.3 \
 -cosineT=0.3 -type=intdocvector
</pre>

<p>
The cosine similarity of two document vectors is computed from the normalized inner product. This program
outputs all article pairs with the cosine similarity above 0.3, which corresponds to a Hamming distance of
400 out of 1000 bits. This relation is due to the theory behind LSH and how it is used to approximate similarity.
We consider this output as ground truth, so we can assess the error due to (i) the signature generation
process, and (ii) the randomized sliding window algorithm (details about this evaluation is provided
in <a href="publications/Ture_etal_SIGIR2011.pdf">Ture et al's SIGIR'11 paper</a>).
</p>

<p>
Using the provided script <code>etc/eval.pl</code>, we can compute precision and recall of the filtered output
with respect to the ground truth:
</p>

<pre class="code">
hadoop dfs -get pwsim.enwiki.index/similardocs_random_maxdst=400_D=1000_Q=300_B=2000-filtered/part-00000 ./pwsim-filtered_400-1000-300-2000
hadoop dfs -get pwsim.enwiki.index/groundtruth_T=0.3/part-00000 ./ground_0.3
cd etc/
perl eval.pl ../ground_0.3 ../pwsim-filtered_400-1000-300-2000 
</pre>

<p>
Since the output of our algorithm mixes two kinds of error (mentioned above), we can also generate an upperbound output by running brute force on signatures. This allows
us to measure the error due to the sliding window algorithm in isolation. For this, we first need to sample signatures (like we did for int doc vectors):
</p>

<pre class="code">
etc/hadoop-cluster.sh ivory.lsh.eval.SampleSignatures \
 pwsim.dewiki.index/signatures-random_D=1000 \
 pwsim.dewiki.index/signatures-random_D=1000_sample=1000 \
 random -1 pwsim.dewiki.index/sample-docnos
</pre>

Now, we can run the same brute-force program to output pairs based on Hamming distance (as opposed to true cosine similarity):

<pre class="code">
etc/hadoop-cluster.sh ivory.lsh.eval.BruteForcePwsim \
 -input=pwsim.enwiki.index/signatures-random_D=1000 \
 -sample=pwsim.dewiki.index/signatures-random_D=1000_sample=1000/part-00000 \
 -output=pwsim.enwiki.index/upperbound_maxdst=400 \
 -cosineT=400 -type=signature
</pre>

<p>
Again, using the provided script <code>etc/eval.pl</code>, we can compute precision and recall of the filtered output
with respect to this upperbound:
</p>

<pre class="code">
hadoop dfs -get pwsim.enwiki.index/upperbound_maxdst=400/part-00000 ./upper_400
cd etc/
perl eval.pl ../upper_400 ../pwsim-filtered_400-1000-300-2000
</pre>

<p>
Another useful tool is to convert the output of the sliding window algorithm (i.e., [key: (German document id, English document id), value: distance]) to
a more human-friendly format, such as [key: German document title, value: English document title]. 
</p>

<p>
First, let's combine all of the <code>SequenceFile</code>s under <code>pwsim.enwiki.index/similardocs_random_maxdst=400_D=1000_Q=300_B=2000</code> 
into a single file for easy loading:
</p>

<pre class="code">
hdfs dfs -mkdir pwsim.results
etc/hadoop-cluster.sh edu.umd.cloud9.util.CombineSequenceFiles \
 pwsim.enwiki.index/similardocs_random_maxdst=400_D=1000_Q=300_B=2000 \
 pwsim.results/similardocs_random_maxdst=400_D=1000_Q=300_B=2000.single \
 100 1 edu.umd.cloud9.io.pair.PairOfInts org.apache.hadoop.io.IntWritable sequence
</pre>

<p>Now, we can convert the docno-based output format to title pairs:</p>
<pre class="code">
etc/hadoop-cluster.sh ivory.lsh.eval.Docnos2Titles \
 -pwsim_output=pwsim.results/similardocs_random_maxdst=400_D=1000_Q=300_B=2000.single/part-00000 \
 -output=pwsim.results/similardocs_random_maxdst=400_D=1000_Q=300_B=2000-filtered.titles \
 -e_collection=pwsim.enwiki.compressed \
 -f_collection=pwsim.dewiki.compressed \
 -f_lang=de -e_lang=en -docnos=pwsim.dewiki.index/sample-docnos
</pre>

<p>Once the job completes, we can check out some of the similar Wikipedia titles:</p>

<pre class="code">
hdfs dfs -cat pwsim.results/similardocs_random_maxdst=400_D=1000_Q=300_B=2000-filtered.titles/part-00000 | less
</pre>

<p>Although most of the pairs should be about relevant topics, you'll probably notice that some of the pairs 
look wrong. This is expected, and may be due to several reasons, analyzed in more detail 
in <a href="publications/Ture_etal_SIGIR2011.pdf">Ture et al's SIGIR'11 paper</a>.</p>

Congratulations! You have completed the first phase of our cross-language similarity pipeline. 
In the next phase, we show <a href="bitext-tutorial.html">how to find parallel sentence pairs</a> 
from the output of this phase. For any further questions and comments, feel free to 
contact <a href="mailto:ferhan.ture@gmail.com">Ferhan Ture</a>.
<p>
<p>
  </div>



    <!-- Footer
    ================================================== -->
    <footer class="footer">
      <div class="container">
        <p class="pull-right"><a href="#">Back to top</a></p>
        <p>Designed using <a href="http://twitter.github.com/bootstrap/">Bootstrap</a>.</p>
        <p>Code licensed under <a href="http://www.apache.org/licenses/LICENSE-2.0" target="_blank">Apache License v2.0</a>, documentation under <a href="http://creativecommons.org/licenses/by/3.0/">CC BY 3.0</a>.</p>
      </div>
    </footer>

    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="assets/js/jquery.js"></script>
    <script src="assets/js/google-code-prettify/prettify.js"></script>
    <script src="assets/js/bootstrap-transition.js"></script>
    <script src="assets/js/bootstrap-alert.js"></script>
    <script src="assets/js/bootstrap-modal.js"></script>
    <script src="assets/js/bootstrap-dropdown.js"></script>
    <script src="assets/js/bootstrap-scrollspy.js"></script>
    <script src="assets/js/bootstrap-tab.js"></script>
    <script src="assets/js/bootstrap-tooltip.js"></script>
    <script src="assets/js/bootstrap-popover.js"></script>
    <script src="assets/js/bootstrap-button.js"></script>
    <script src="assets/js/bootstrap-collapse.js"></script>
    <script src="assets/js/bootstrap-carousel.js"></script>
    <script src="assets/js/bootstrap-typeahead.js"></script>
    <script src="assets/js/bootstrap-affix.js"></script>

  </body>
</html>

